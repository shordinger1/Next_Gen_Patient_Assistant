# 扩充中文词表
使用的数据包含中、英文无监督数据和平行语料，在语料上重新训练 spm tokenizer，在中文上获得字词结合的分词效果。

## 构建 tokenizer
1. 首先在大规模中英文领域语料上训练 SPM，词表大小依据训练集指定，一般大于2000，使tokenizer可以完整切分`病毒`、`感染`、`慕容复`等领域词

- 训练脚本：https://github.com/shibing624/MedicalGPT/blob/main/build_domain_tokenizer.py
- sentencepiece训练参考：https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb

2. 扩充百川中文词表，该词表中文识字率较高，用于提高简繁体汉字的识字率；根据[结巴分词](https://github.com/fxsjy/jieba)词频前20000的词表扩充中文词，提高专名切分效果

- 合并词表脚本：https://github.com/shibing624/MedicalGPT/blob/main/merge_tokenizers.py

效果示例：

原文： ```this is a test, hello world. thisisatesthelloworld, 
慕容复来到河边，姑苏慕容氏在外面丢了人。
1号店一周岁了，我们一古脑儿买了10斤零食。
巴塞罗那足球俱乐部简称巴萨（Barça），是一家位于西班牙加泰罗尼亚巴塞罗那的足球俱乐部，于1899年由瑞士企业家胡安·甘伯所创立，世界球坛顶级足球俱乐部之一。俱乐部主场可容纳接近十万名观众，是全欧洲最大及世界第二大的足球场。
白日依山尽，黄河入海流。欲穷千里目，更上一层楼。```

Medical LLaMA tokenizer: ```['▁this', '▁is', '▁a', '▁test', ',', '▁hello', '▁world', '.', '▁this', 'isat', 'est', 'h', 'ellow', 'orld', ',', '▁', '<0x0A>', '慕容复', '来到', '河边', '，', '姑', '苏', '慕容', '氏', '在外', '面', '丢', '了', '人', '。', '<0x0A>', '1', '号', '店', '一周', '岁', '了', '，', '我们', '一', '古', '脑', '儿', '买了', '1', '0', '斤', '零食', '。', '<0x0A>', '巴', '塞', '罗', '那', '足球', '俱乐部', '简称', '巴', '萨', '（', 'Bar', 'ça', '）', '，', '是一家', '位于', '西班牙', '加', '泰', '罗', '尼亚', '巴', '塞', '罗', '那', '的', '足球', '俱乐部', '，', '于', '1', '8', '9', '9', '年', '由', '瑞士', '企业家', '胡', '安', '·', '甘', '伯', '所', '创立', '，', '世界', '球', '坛', '顶级', '足球', '俱乐部', '之一', '。', '俱乐部', '主场', '可', '容纳', '接近', '十万', '名', '观众', '，', '是', '全', '欧洲', '最大', '及', '世界', '第二', '大的', '足球', '场', '。', '<0x0A>', '白', '日', '依', '山', '尽', '，', '黄河', '入', '海', '流', '。', '欲', '穷', '千里', '目', '，', '更', '上一', '层', '楼', '。']```



LLaMA 原始 tokenizer: ```['▁this', '▁is', '▁a', '▁test', ',', '▁hello', '▁world', '.', '▁this', 'isat', 'est', 'h', 'ellow', 'orld', ',', '▁', '<0x0A>', '<0xE6>', '<0x85>', '<0x95>', '容', '复', '来', '到', '河', '边', '，', '<0xE5>', '<0xA7>', '<0x91>', '<0xE8>', '<0x8B>', '<0x8F>', '<0xE6>', '<0x85>', '<0x95>', '容', '氏', '在', '外', '面', '<0xE4>', '<0xB8>', '<0xA2>', '了', '人', '。', '<0x0A>', '1', '号', '店', '一', '周', '<0xE5>', '<0xB2>', '<0x81>', '了', '，', '我', '们', '一', '古', '<0xE8>', '<0x84>', '<0x91>', '<0xE5>', '<0x84>', '<0xBF>', '<0xE4>', '<0xB9>', '<0xB0>', '了', '1', '0', '<0xE6>', '<0x96>', '<0xA4>', '<0xE9>', '<0x9B>', '<0xB6>', '食', '。', '<0x0A>', '巴', '<0xE5>', '<0xA1>', '<0x9E>', '<0xE7>', '<0xBD>', '<0x97>', '那', '足', '球', '<0xE4>', '<0xBF>', '<0xB1>', '乐', '部', '<0xE7>', '<0xAE>', '<0x80>', '称', '巴', '<0xE8>', '<0x90>', '<0xA8>', '（', 'Bar', 'ça', '）', '，', '是', '一', '家', '位', '于', '西', '<0xE7>', '<0x8F>', '<0xAD>', '<0xE7>', '<0x89>', '<0x99>', '加', '泰', '<0xE7>', '<0xBD>', '<0x97>', '<0xE5>', '<0xB0>', '<0xBC>', '<0xE4>', '<0xBA>', '<0x9A>', '巴', '<0xE5>', '<0xA1>', '<0x9E>', '<0xE7>', '<0xBD>', '<0x97>', '那', '的', '足', '球', '<0xE4>', '<0xBF>', '<0xB1>', '乐', '部', '，', '于', '1', '8', '9', '9', '年', '由', '<0xE7>', '<0x91>', '<0x9E>', '士', '<0xE4>', '<0xBC>', '<0x81>', '业', '家', '<0xE8>', '<0x83>', '<0xA1>', '安', '·', '<0xE7>', '<0x94>', '<0x98>', '<0xE4>', '<0xBC>', '<0xAF>', '所', '创', '立', '，', '世', '界', '球', '<0xE5>', '<0x9D>', '<0x9B>', '<0xE9>', '<0xA1>', '<0xB6>', '<0xE7>', '<0xBA>', '<0xA7>', '足', '球', '<0xE4>', '<0xBF>', '<0xB1>', '乐', '部', '之', '一', '。', '<0xE4>', '<0xBF>', '<0xB1>', '乐', '部', '主', '场', '可', '容', '<0xE7>', '<0xBA>', '<0xB3>', '接', '近', '十', '万', '名', '<0xE8>', '<0xA7>', '<0x82>', '<0xE4>', '<0xBC>', '<0x97>', '，', '是', '全', '<0xE6>', '<0xAC>', '<0xA7>', '洲', '最', '大', '及', '世', '界', '第', '二', '大', '的', '足', '球', '场', '。', '<0x0A>', '白', '日', '<0xE4>', '<0xBE>', '<0x9D>', '山', '<0xE5>', '<0xB0>', '<0xBD>', '，', '黄', '河', '入', '海', '流', '。', '<0xE6>', '<0xAC>', '<0xB2>', '<0xE7>', '<0xA9>', '<0xB7>', '千', '里', '目', '，', '更', '上', '一', '<0xE5>', '<0xB1>', '<0x82>', '<0xE6>', '<0xA5>', '<0xBC>', '。']```

## 扩词表后的增量预训练


- 扩词表后的增量预训练，PT阶段加上`--modules_to_save embed_tokens,lm_head`参数，后续SFT等阶段不用加
- 其他训练参数见 https://github.com/shibing624/MedicalGPT/blob/main/docs/training_params.md